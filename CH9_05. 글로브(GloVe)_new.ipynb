{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7125f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0140bfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1fab0136610>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d0b850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "# content 데이터만 가져옴\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# (Audio), (Laughter) 등 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해 NLTK를 이용하여 문장 토큰화 수행\n",
    "sent_text = sent_tokenize(content_text)\n",
    "sent_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58da090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 대해 구두점을 제거하고 대문자를 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r'[^a-z0-0]', ' ', string.lower())\n",
    "    normalized_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0df71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n"
     ]
    }
   ],
   "source": [
    "# 각 문장에 대해 NLTK를 이용하여 단어 토큰화 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebe9f4",
   "metadata": {},
   "source": [
    "### GloVe 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46722795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dca6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터로부터 GloVe에서 사용할 동시등장행렬 생성\n",
    "corpus.fit(result, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae63d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove(no_components=100, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9baffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 20 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "# 학습에 이용할 쓰레드 개수 4, 에코크 20\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36294d5e",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2154d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9565617376988225), ('guy', 0.889639917762371), ('girl', 0.8569227434157962), ('kid', 0.8355838534152334)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(\"man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eeb706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.9402652404600705), ('kid', 0.8416383971200846), ('woman', 0.8406037110104396), ('man', 0.8174028308008443)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar('boy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f142ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('harvard', 0.8966821120517038), ('mit', 0.863381827933563), ('cambridge', 0.8446023319763226), ('stanford', 0.8294566078706815)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar('university'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e3a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
