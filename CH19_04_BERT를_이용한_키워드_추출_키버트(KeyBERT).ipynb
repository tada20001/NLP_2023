{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHwpbZLqQTnOCpbRD2S7a4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tada20001/NLP_2023/blob/main/CH19_04_BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%82%A4%EC%9B%8C%EB%93%9C_%EC%B6%94%EC%B6%9C_%ED%82%A4%EB%B2%84%ED%8A%B8(KeyBERT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqfEmrbxcuxm",
        "outputId": "9a669506-262d-4849-d2ba-8cbac56d0f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# 우선 SBERT를 위한 패키지 설치 필요\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 기본 KeyBERT\n",
        "-----------------"
      ],
      "metadata": {
        "id": "-qk37xu9c-QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "BjLdumJgc9Vz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of \n",
        "         learning a function that maps an input to an output based \n",
        "         on example input-output pairs.[1] It infers a function \n",
        "         from labeled training data consisting of a set of \n",
        "         training examples.[2] In supervised learning, each \n",
        "         example is a pair consisting of an input object \n",
        "         (typically a vector) and a desired output value (also \n",
        "         called the supervisory signal). A supervised learning \n",
        "         algorithm analyzes the training data and produces an \n",
        "         inferred function, which can be used for mapping new \n",
        "         examples. An optimal scenario will allow for the algorithm \n",
        "         to correctly determine the class labels for unseen \n",
        "         instances. This requires the learning algorithm to  \n",
        "         generalize from the training data to unseen situations \n",
        "         in a 'reasonable' way (see inductive bias).\n",
        "      \"\"\""
      ],
      "metadata": {
        "id": "e9sOhBQzdIQh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn의 CountVectorizer를 사용하여 단어 추출. n_gram_range 인자를 사용하여 3개의 단어를 한 묶음으로 간주하는 trigram 추출함 "
      ],
      "metadata": {
        "id": "X34cdLQhdoTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_range = (3, 3)\n",
        "stop_words = \"english\"\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('trigram 개수:', len(candidates))\n",
        "print('trigram 단어묶음 다섯개만 출력:', candidates[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVWJQBOdlZ5",
        "outputId": "6a9a7048-1f30-4752-8070-634fac5e75e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram 개수: 72\n",
            "trigram 단어묶음 다섯개만 출력: ['algorithm analyzes training' 'algorithm correctly determine'\n",
            " 'algorithm generalize training' 'allow algorithm correctly'\n",
            " 'analyzes training data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로 문서로부터 추출한 키워드들을 SBERT를 통해 수치화함(인코딩)\n"
      ],
      "metadata": {
        "id": "GNbl2klmixD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "doc_embedding = model.encode([doc])  # 전체 문서 인코딩\n",
        "candidate_embeddings = model.encode(candidates)  # trigram 키워드 인코딩"
      ],
      "metadata": {
        "id": "_4RvuvJbgAGL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_embedding.shape)  # 768개 숫자로 인코딩됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq2kFKQ7jKqy",
        "outputId": "6a29891b-aee3-4fc1-8434-44fcb0f3a45d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFe1ko4vYmj4",
        "outputId": "bb12938a-a914-431d-f3f2-d93c8bb79481"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.1052822"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_embeddings.shape  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2dMesCajRWD",
        "outputId": "f92d3776-750c-42b5-b8a3-63bf15c99857"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 문서와 가장 유사한 키워드를 추출함. 키워드 중에 문서와 가장 유사한 키워드는 문서를 대표하는 키워드로 가정하고, 상위 5개 키워드를 출력함."
      ],
      "metadata": {
        "id": "YaDxYX3_jrw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK-BGHwUjX1x",
        "outputId": "e75839f3-2aa3-4b37-be27-45173305fa92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithm analyzes training',\n",
              " 'learning algorithm generalize',\n",
              " 'learning machine learning',\n",
              " 'learning algorithm analyzes',\n",
              " 'algorithm generalize training']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5개의 키워드가 출력되었는데 의미가 비슷해 보임. 당연히 이 키워드들이 문서를 가장 잘 나타내기 때문임.\n",
        "\n",
        "좀더 다양한 의미의 키워드들은 문서를 잘 나타낼 가능성이 적을 수도 있음. 따라서 키워드의 정확성과 다양성 사이에는 균형이 필요함.\n",
        "\n",
        "여기서는 다양한 키워드들을 얻기 위해 두가지 알고리즘을 사용함.\n",
        "\n",
        "* Max Sum Similarity\n",
        "* Maximal Marginal Relevance\n",
        "\n",
        "\n",
        "### 2. Max Sum Similarity\n",
        "--------------------\n",
        "키워드간 거리가 최대화되는 키워드 쌍을 정의하는 함수를 이용함. 키워드간 유사성을 최소화하면서 문서와의 유사성을 극대화한다는 것임."
      ],
      "metadata": {
        "id": "KtR2O6Hdn_ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "  # 문서와 각 키워드들 간의 유사도\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "\n",
        "  # 각 키워드들 간의 유사도\n",
        "  distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "\n",
        "  # 코사인 유사도에 기반하여 키워드 중 상위 top_n개의 단어를 선택\n",
        "  words_idx = list(distances.argsort()[0][-nr_candidates:])  # 문서와 키워드간 유사도가 큰 n개 키워드 선택\n",
        "  words_vals = [candidates[index] for index in words_idx]  # 문서와 키워드간 유사도 가 큰 n개 키워드의 유사도 값 추출\n",
        "  distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)] # 선정된 키워드들간 유사도 추출\n",
        "\n",
        "  # 선택한 키워드들 중 가장 덜 유사한 키워드들 간 조합 계산\n",
        "  min_sim = np.inf\n",
        "  candidate = None\n",
        "  for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "    sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "    if sim < min_sim:\n",
        "      candidate = combination\n",
        "      min_sim = sim\n",
        "  return [words_vals[idx] for idx in candidate]  # 선정했던 10개 키워드 중 상호간 유사도가 적은 키워드 추출"
      ],
      "metadata": {
        "id": "xrJD4wConsEk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 10개 키워드를 선택하고 이 중 서로 가장 유사성이 낮은 5개를 선택함\n",
        "# 낮은 nr_candidates를 설정하면 결과는 출력된 키워드 5개는 기존의 코사인 유사도만 사용한 것과 아주 유사한 결과를 도출\n",
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnJZt1hDsHV9",
        "outputId": "600ffb99-bdbe-4de1-d0a9-5c4dcea064ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['requires learning algorithm',\n",
              " 'signal supervised learning',\n",
              " 'learning function maps',\n",
              " 'algorithm analyzes training',\n",
              " 'learning machine learning']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 20개 키워드 중 유사하지 안은 키워드 5개 뽑기\n",
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLlOdYDkR5qS",
        "outputId": "2e1b8838-36fc-4eba-dd00-06dff69e55eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set training examples',\n",
              " 'generalize training data',\n",
              " 'requires learning algorithm',\n",
              " 'supervised learning algorithm',\n",
              " 'learning machine learning']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Maximal Marginal Relevance\n",
        "----------------\n",
        "결과를 다양화하는 다른 방법으로 MMR(Maximal Marginal Relevance)이 있음. MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화함. \n",
        "\n",
        "* 참고자료 : EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) ==> 키워드를 다양화하는데 사용할 수 있는 알고리즘\n",
        "\n",
        "프로세스\n",
        "\n",
        "* step 1) 문서와 가장 유사한 키워드/키 프레이즈를 선택\n",
        "* step 2) 문서와 유사하고 이미 선택된 키워드/키 프레이즈와 유사하지 않은 새로운 후보 선택\n",
        "\n",
        "step2 계속 반복함\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pPGmWhYFS-_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "  # 문서와 각 키워드 간 유사도를 구한 리스트\n",
        "  word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "\n",
        "  # 각 키워드 간 유사도\n",
        "  word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "\n",
        "  # 문서와 가장 높은 유사도를 가진 키워드 인덱스 추출\n",
        "  keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "\n",
        "  # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스\n",
        "  candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "  # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래 반복\n",
        "  for _ in range(top_n - 1):\n",
        "    candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "    target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "    # MMR 계산\n",
        "    mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "    mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "    # keywords & candidates 업데이트\n",
        "    keywords_idx.append(mmr_idx)\n",
        "    candidates_idx.remove(mmr_idx)\n",
        "\n",
        "  return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "_7uD3CZkS5WD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 만약 상대적으로 낮은  diversity 값을 설정할 수록, 결과는 기존의 코사인 유사도만 사용한 것과 유사한 결과를 보임\n",
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqggCjba0sfF",
        "outputId": "3006b924-3b40-4625-c339-8186ee95fc9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithm generalize training',\n",
              " 'supervised learning algorithm',\n",
              " 'learning machine learning',\n",
              " 'learning algorithm analyzes',\n",
              " 'learning algorithm generalize']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odli9Hvn1ZFP",
        "outputId": "6367df82-2f24-4b09-80b8-e16a6ac4abd9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithm generalize training',\n",
              " 'labels unseen instances',\n",
              " 'new examples optimal',\n",
              " 'determine class labels',\n",
              " 'supervised learning algorithm']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2knawCVW2WTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}