{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa05b2a",
   "metadata": {},
   "source": [
    "페이스북에서 개발한 FastText으로, Word2Vec의 확장 버전.\n",
    "\n",
    "가장 큰 차이점은 Word2Vec은 단어를 더 이상 구분할 수 없는 최소단위로 본다면, FastText는 하나의 단어 안에 여러 단어들이 존재하는 것으로 간주함. 즉 서드워드(subword)를 고려함\n",
    "\n",
    "### 1. 서브워드(subword) 학습\n",
    "-------------------\n",
    "FastText에서는 각 단어가 글자단위 n-gram의 구성을 취함. n을 몇으로 결정할지에 따라 단어들이 얼마나 분리되는지 결정됨. \n",
    "\n",
    "\n",
    "* 예를 들어 n을 3으로 잡은 tri-gram의 경우,\n",
    "\n",
    "    * apple은 app, ppl, ple로 분리하고 ---> 이들을 벡터로 만듦\n",
    "    * 정확하게는 시작과 끝을 의미하는 <, >를 도입하여 다음과 같이 토큰을 만들어 벡터화함"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fcc35c4",
   "metadata": {},
   "source": [
    "# n = 3인 경우\n",
    "<ap, app, ppl, ple, le> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f75dc",
   "metadata": {},
   "source": [
    "그리고 추가로 하나를 더 벡터화하는데, 전체 단어에 <, 와 >를 붙인 토큰임. 즉 다음과 같이 6개의 토큰을 벡터화함"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa9d7d84",
   "metadata": {},
   "source": [
    "# n = 3인 경우\n",
    "<ap, app, ppl, ple, le>, <apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c3a0d",
   "metadata": {},
   "source": [
    "실제 사용시에는 n의 최소값과 최대값 범위를 설정하는데, 기본값으로는 3과 6이 설정되어져 있음. 즉, 다음과 같은 결과가 나옴"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de67e772",
   "metadata": {},
   "source": [
    "# n = 3 ~ 6인 경우\n",
    "<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6f5cc",
   "metadata": {},
   "source": [
    "이 서브워드들을 벡터화하는 것으로 이 단어에 대해 Word2Vec을 수행한다는 의미임. 위와 같이 내부단어들의 벡터값을 얻었다면 단어 apple의 벡터값은 위 벡터값들의 총합으로 구성됨"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76fab6e2",
   "metadata": {},
   "source": [
    "apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58378542",
   "metadata": {},
   "source": [
    "### 2. 모르는 단어(Out of Vocabulary, OOV)에 대한 대응\n",
    "-----------------------------\n",
    "FastText의 인공신경망을 학습한 후에 데이터셋의 모든 단어의 각 n-gram에 대해 워드임베딩이 됨. 이렇게 되면 장점은 데이터셋만 충분하다면 위와 같은 내부단어(subword)를 통해 모르는 단어에 대해서도 다른 단어와의 유사도를 계산할 수 있음\n",
    "\n",
    "\n",
    "### 3. 단어집합 내 빈도수가 적은 단어(Rare Word)에 대한 대응\n",
    "-------------------------------\n",
    "Word2Vec의 경우, 등장빈도가 적은 단어(rare words)에 대해서는 임베딩 정확도가 높지 않다는 단점이 있음. 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않음\n",
    "\n",
    "FastText의 경우, 단어가 희귀하더라도 비교적은 높은 임베딩 값을 얻을 수 있음. FastText가 노이즈가 ㅁ낳은 코퍼스에서 강점을 가지는 이유도 여기에 있음\n",
    "\n",
    "모든 훈련 코퍼스에 오타나 맞춤법이 틀린 단어가 없으면 이상적이겠지만, 실제 많은 비정형 데이터에는 오타가 섞여 있음. 오타가 섞인 단어는 등장빈도가 적어 일정의 희귀단어가 됨. \n",
    "\n",
    "FastText에서는 이러한 단어들에도 대응 가능\n",
    "\n",
    "\n",
    "### 4. Word2Vec vs. FastText 비교\n",
    "-------------------------------\n",
    "#### 1) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7ec3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1a379886ac0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩 및 전처리\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337dc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='utf8')\n",
    "target_text = etree.parse(targetXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ff96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content 데이터만 추출\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36dded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배경음 등 노이즈 제거\n",
    "regex =  r'\\([^)]*\\)'\n",
    "content_text = re.sub(regex, '', parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4119cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 수행\n",
    "sent_text = sent_tokenize(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd16fa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f333c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 제거, 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r'[^a-z0-9]+', \" \", string.lower())  # 빈칸으로 대체\n",
    "    normalized_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f525a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 대한 토큰화 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f084ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273380\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4099749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### word2vec 훈련시키기\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ef526f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 저장 및 로드\n",
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793ea2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3336\\3181498350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'electrofishing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;31m# compute the weighted average of all keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m         all_keys = [\n\u001b[0;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "loaded_model.most_similar('electrofishing') # 단어가 없기 때문에 에러 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a715197",
   "metadata": {},
   "source": [
    "#### 2) FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca9fab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b26b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)  # sg: skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba11d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8636344075202942),\n",
       " ('electrolyte', 0.8625998497009277),\n",
       " ('electroshock', 0.8513526320457458),\n",
       " ('electro', 0.8485394716262817),\n",
       " ('electroencephalogram', 0.8393594026565552),\n",
       " ('electrochemical', 0.8290709853172302),\n",
       " ('electrogram', 0.8269733190536499),\n",
       " ('electrons', 0.8167208433151245),\n",
       " ('electron', 0.8142461776733398),\n",
       " ('electric', 0.8126084208488464)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98630028",
   "metadata": {},
   "source": [
    "Word2Vec에서는 유사한 단어를 찾아내지 못했으나, FastText는 유사한 단어를 계산해서 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edc338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
