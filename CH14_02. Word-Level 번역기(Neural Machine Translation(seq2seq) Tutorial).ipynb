{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999b0af",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리\n",
    "\n",
    "* 훈련데이터로 병렬 코퍼스 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d75843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fcaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fra-eng. zip 파일 로드 : 샘플 33000개만 추출\n",
    "num_samples = 33000\n",
    "\n",
    "# 전처리 함수 구현\n",
    "def to_ascii(s):\n",
    "    # 프랑스 악센트 삭제\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 악센트 제거 함수 호출\n",
    "    sent = to_ascii(sent.lower())\n",
    "    \n",
    "    # 단어와 구두점 사이에 공백 추가\n",
    "    ## ex) \"I am a student.\" => \"I am a student .\"\n",
    "    sent = re.sub(r'([?.!,¿])', r' \\1', sent)\n",
    "    \n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r'[^a-zA-Z!.?]+', r' ', sent)\n",
    "    \n",
    "    # 여러 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcc086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장: Have you had dinner?\n",
      "전처리 후 영어 문장: have you had dinner ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장:', en_sent)\n",
    "print('전처리 후 영어 문장:', preprocess_sentence(en_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f771f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 프랑스어 문장: Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장: avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "print('전처리 전 프랑스어 문장:', fr_sent)\n",
    "print('전처리 후 프랑스어 문장:', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07124b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터에 대해 전처리 수행\n",
    "# 교사 강요를 사용할 예정이므로, 훈련시 사용할 디코더의 입력시퀀스와 실제값, \n",
    "# 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 지정\n",
    "# 입력시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, \n",
    "# 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가함\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "    \n",
    "    with open('fra.txt', 'r', encoding='utf8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "            \n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "            \n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "            \n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "            \n",
    "            if i == num_samples - 1:  # 33,000개만 추출\n",
    "                break\n",
    "                \n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb42f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931faa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력: [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력: [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 출력: [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력:', sents_en_in[:5])\n",
    "print('디코더의 입력:', sents_fra_in[:5])\n",
    "print('디코더의 출력:', sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd66d9a",
   "metadata": {},
   "source": [
    "* 여기에서 디코더의 입력 데이터는 교사강요에 사용됨\n",
    "\n",
    "---\n",
    "###### 케라스 토크나이저를 통해 단어집합생성 --> 정수인코딩 --> 패딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93dc3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_padding(data):\n",
    "    tokenizer = Tokenizer(filters=\"\", lower=False)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    sequenced_data = tokenizer.texts_to_sequences(data)\n",
    "    padded = pad_sequences(sequenced_data, padding='post')\n",
    "    return padded, tokenizer\n",
    "\n",
    "encoder_input, tokenizer_en = process_padding(sents_en_in)\n",
    "decoder_input, tokenizer_fra = process_padding(sents_fra_in)\n",
    "decoder_target, _ = process_padding(sents_fra_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90091d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력크기:  (33000, 8)\n",
      "디코더의 입력크기:  (33000, 16)\n",
      "디코더의 레이블 크기:  (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력크기: ',  encoder_input.shape)\n",
    "print('디코더의 입력크기: ',  decoder_input.shape)\n",
    "print('디코더의 레이블 크기: ',  decoder_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59b068",
   "metadata": {},
   "source": [
    "* 샘플은 총 33,000개이며, 영어 문장의 길이는 8, 프랑스어 문장 길이는 16임\n",
    "* 단어 집합 크기를 정의함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d740a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4620 8053\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(src_vocab_size, tar_vocab_size) # 단어집합의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12035873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 디셔너리 만듦\n",
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d64301f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [  838 19925 28428 ... 26048 24876  7157]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 분리하기 전 shuffle 필요\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e0a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0642f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54 83  1  0  0  0  0  0]\n",
      "[  1 160  37  12   8  92   2   0   0   0   0   0   0   0   0   0]\n",
      "[160  37  12   8  92   2   1   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# check!!\n",
    "print(encoder_input[30997])\n",
    "print(decoder_input[30997])\n",
    "print(decoder_target[30997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaa1435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 분리 --> 10%\n",
    "n_of_val = int(33000 / 10)\n",
    "n_of_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "819a7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d245c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 8)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 8)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885336e",
   "metadata": {},
   "source": [
    "### 2. 모델 설계 : 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf0f75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "101809c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 차원과 LSTM 은닉상태의 크기를 64로 사용\n",
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd2dab",
   "metadata": {},
   "source": [
    "* 인코더 설계 : Masking은 패딩 토큰인 숫자 0의 경우, 연산을 제외하는 역할 수행, 인코더의 내부상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정\n",
    "* LSTM : state_h, state_c를 리턴받고, 이 두가지 상태 모두를 디코더로 전달(컨텍스트 벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c39470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5da49",
   "metadata": {},
   "source": [
    "* 디코더는 인코더의 마지막 은닉상태로부터 초기 은닉상태를 얻음\n",
    "* inital_state의 인자값으로 encoder_states를 주는 코드가 이에 해당함\n",
    "* 디코더도 은닉상태, 셀상태를 리턴하기는 하지만, 훈련과정에서는 사용하지 않음\n",
    "* seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류문제를 풀고 있음\n",
    "* 따라서 출력층으로 소프트맥스 함수와 손실함수를 크로스 엔트로피 함수를 사용함\n",
    "\n",
    "---\n",
    "* categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 함\n",
    "* 그러나 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해 다중 클래스 분류 문제를 풀고자 하는 경우에는 'categorical_crossentropy'를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4839fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, embedding_dim) # embedding layer\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state=True, \n",
    "# 모든 시점에 대해 단어를 예측하기 위해 return_sequences=True\n",
    "decoder_lstm = LSTM(hidden_units, return_state=True, return_sequences=True)\n",
    "\n",
    "# 인코더의 은닉상태를 초기 은닉상태(initial state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model. compile(optimizer='adam', loss='sparse_categorical_crossentropy', \\\n",
    "               metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b778670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training : 120개 배치크기, 50 에포크 학습\n",
    "29700 / 120  # 247번 나눠서 진행하고 이 프로세스를 50번 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d46bd7b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "248/248 [==============================] - 55s 223ms/step - loss: 3.2678 - acc: 0.6122 - val_loss: 1.9316 - val_acc: 0.6393\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 73s 293ms/step - loss: 1.7901 - acc: 0.7158 - val_loss: 1.6932 - val_acc: 0.7422\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 121s 486ms/step - loss: 1.6039 - acc: 0.7483 - val_loss: 1.5421 - val_acc: 0.7554\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 125s 502ms/step - loss: 1.4744 - acc: 0.7595 - val_loss: 1.4425 - val_acc: 0.7633\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 121s 489ms/step - loss: 1.3774 - acc: 0.7734 - val_loss: 1.3551 - val_acc: 0.7831\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 117s 471ms/step - loss: 1.2921 - acc: 0.7902 - val_loss: 1.2874 - val_acc: 0.7933\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 113s 456ms/step - loss: 1.2264 - acc: 0.7991 - val_loss: 1.2349 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 117s 472ms/step - loss: 1.1672 - acc: 0.8093 - val_loss: 1.1857 - val_acc: 0.8115\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 119s 480ms/step - loss: 1.1127 - acc: 0.8170 - val_loss: 1.1431 - val_acc: 0.8178\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 115s 464ms/step - loss: 1.0643 - acc: 0.8239 - val_loss: 1.1084 - val_acc: 0.8220\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 116s 468ms/step - loss: 1.0212 - acc: 0.8284 - val_loss: 1.0775 - val_acc: 0.8255\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 119s 481ms/step - loss: 0.9806 - acc: 0.8325 - val_loss: 1.0488 - val_acc: 0.8288\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 115s 463ms/step - loss: 0.9442 - acc: 0.8365 - val_loss: 1.0237 - val_acc: 0.8326\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 113s 458ms/step - loss: 0.9101 - acc: 0.8404 - val_loss: 1.0019 - val_acc: 0.8347\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 113s 454ms/step - loss: 0.8778 - acc: 0.8439 - val_loss: 0.9805 - val_acc: 0.8372\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 115s 463ms/step - loss: 0.8479 - acc: 0.8468 - val_loss: 0.9630 - val_acc: 0.8391\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 112s 450ms/step - loss: 0.8195 - acc: 0.8497 - val_loss: 0.9449 - val_acc: 0.8408\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 109s 438ms/step - loss: 0.7929 - acc: 0.8524 - val_loss: 0.9296 - val_acc: 0.8431\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 108s 437ms/step - loss: 0.7669 - acc: 0.8553 - val_loss: 0.9145 - val_acc: 0.8435\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 113s 456ms/step - loss: 0.7418 - acc: 0.8579 - val_loss: 0.8996 - val_acc: 0.8458\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 106s 429ms/step - loss: 0.7174 - acc: 0.8606 - val_loss: 0.8857 - val_acc: 0.8477\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 109s 439ms/step - loss: 0.6943 - acc: 0.8634 - val_loss: 0.8722 - val_acc: 0.8487\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 112s 453ms/step - loss: 0.6718 - acc: 0.8662 - val_loss: 0.8613 - val_acc: 0.8501\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 107s 432ms/step - loss: 0.6504 - acc: 0.8686 - val_loss: 0.8495 - val_acc: 0.8511\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 113s 454ms/step - loss: 0.6299 - acc: 0.8710 - val_loss: 0.8400 - val_acc: 0.8526\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 112s 452ms/step - loss: 0.6101 - acc: 0.8734 - val_loss: 0.8307 - val_acc: 0.8538\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 107s 433ms/step - loss: 0.5916 - acc: 0.8759 - val_loss: 0.8239 - val_acc: 0.8548\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 112s 451ms/step - loss: 0.5736 - acc: 0.8778 - val_loss: 0.8149 - val_acc: 0.8559\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 109s 438ms/step - loss: 0.5565 - acc: 0.8804 - val_loss: 0.8078 - val_acc: 0.8567\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 112s 450ms/step - loss: 0.5395 - acc: 0.8829 - val_loss: 0.8023 - val_acc: 0.8573\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 110s 443ms/step - loss: 0.5239 - acc: 0.8850 - val_loss: 0.7943 - val_acc: 0.8585\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 108s 436ms/step - loss: 0.5084 - acc: 0.8877 - val_loss: 0.7893 - val_acc: 0.8594\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 117s 470ms/step - loss: 0.4940 - acc: 0.8897 - val_loss: 0.7846 - val_acc: 0.8598\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 113s 454ms/step - loss: 0.4794 - acc: 0.8924 - val_loss: 0.7800 - val_acc: 0.8601\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 111s 447ms/step - loss: 0.4654 - acc: 0.8948 - val_loss: 0.7752 - val_acc: 0.8609\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 106s 427ms/step - loss: 0.4524 - acc: 0.8969 - val_loss: 0.7699 - val_acc: 0.8612\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 106s 426ms/step - loss: 0.4403 - acc: 0.8991 - val_loss: 0.7663 - val_acc: 0.8613\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 113s 455ms/step - loss: 0.4279 - acc: 0.9009 - val_loss: 0.7634 - val_acc: 0.8628\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 106s 429ms/step - loss: 0.4158 - acc: 0.9033 - val_loss: 0.7601 - val_acc: 0.8629\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 108s 435ms/step - loss: 0.4045 - acc: 0.9053 - val_loss: 0.7561 - val_acc: 0.8636\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 91s 366ms/step - loss: 0.3933 - acc: 0.9075 - val_loss: 0.7529 - val_acc: 0.8637\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 64s 258ms/step - loss: 0.3835 - acc: 0.9093 - val_loss: 0.7514 - val_acc: 0.8654\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 93s 376ms/step - loss: 0.3730 - acc: 0.9111 - val_loss: 0.7480 - val_acc: 0.8662\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 132s 533ms/step - loss: 0.3635 - acc: 0.9130 - val_loss: 0.7458 - val_acc: 0.8659\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 127s 512ms/step - loss: 0.3546 - acc: 0.9147 - val_loss: 0.7445 - val_acc: 0.8668\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 128s 515ms/step - loss: 0.3457 - acc: 0.9166 - val_loss: 0.7424 - val_acc: 0.8677\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 132s 532ms/step - loss: 0.3373 - acc: 0.9180 - val_loss: 0.7410 - val_acc: 0.8677\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 127s 513ms/step - loss: 0.3292 - acc: 0.9196 - val_loss: 0.7403 - val_acc: 0.8676\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 141s 568ms/step - loss: 0.3213 - acc: 0.9216 - val_loss: 0.7407 - val_acc: 0.8682\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 137s 551ms/step - loss: 0.3134 - acc: 0.9229 - val_loss: 0.7384 - val_acc: 0.8689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1949d595dc0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=120, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6c07f",
   "metadata": {},
   "source": [
    "### 3. seq2seq 기계번역기 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c577b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계\n",
    "## 이전 시점의 상태를 저장할 텐서 wjddml\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련때 사용했던 임베딩 층 재사용\n",
    "dec_em2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_em2, \n",
    "                                                    initial_state=decoder_states_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_input, \n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490de2d",
   "metadata": {},
   "source": [
    "* 테스트 단계의 동작을 위한 decode_sequence 함수 구현\n",
    "* 입력문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉상태와 셀 상태를 리턴함\n",
    "* 두개의 값을 states_value에 저장하고, 디코더의 초기입력으로 \\<sos>를 준비함\n",
    "* 이를 target_seq에 저장함\n",
    "* 이 두가지 입력을 가지고 while문에서 디코더 입력으로 사용\n",
    "* 이러한 디코더는 현재 시점에 대해 예측하게 되며, 현재시점의 예측벡터는 output_tokens, 현재 시점의 은닉상태가 h, 현재시점의 셀 상태가 c임\n",
    "* 예측 벡터로부터 현재시점의 예측 단어인 target_seq를 얻고, h와 c 두개의 값은 states_value에 저장함\n",
    "* while 문의 다음 루프, 즉 두번째 시점의 디코더 입력으로 다시 target_seq와 states_value를 사용함\n",
    "* 이를 현재 시점의 예측단어로 \\<eos>를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복함\n",
    "* 각 시점마다 번역된 단어는 decoded_sentence에 누적되면서 저장되었다가 최종 번역 시퀀스로 리턴됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ce58a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d104ae",
   "metadata": {},
   "source": [
    "* 결과 확인을 위한 함수 만들기\n",
    "* seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어단어를 리턴하는 index_to_src를 통해 영어문장으로 변환함\n",
    "* seq_to_tar는 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar를 통해 프랑스어 문장으로 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f764c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2c20b",
   "metadata": {},
   "source": [
    "* 훈련 데이터 중 임의의 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6abbdf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장:  can i borrow yours ? \n",
      "정답문장:  <sos> puis je emprunter le tien ? \n",
      "번역문장:   puis je emprunter le tien ? <sos> <sos> <sos> <sos>\n",
      "--------------------------------------------------\n",
      "입력문장:  i ran out of gas . \n",
      "정답문장:  <sos> je suis tombee en panne d essence . \n",
      "번역문장:   je suis tombee a l interieur . <sos> <sos> <sos> <sos>\n",
      "--------------------------------------------------\n",
      "입력문장:  i really need this . \n",
      "정답문장:  <sos> j en ai vraiment besoin . \n",
      "번역문장:   j ai besoin d une biere . <sos> <sos> <sos> <sos> ca\n",
      "--------------------------------------------------\n",
      "입력문장:  water the plants . \n",
      "정답문장:  <sos> arrosez les plantes ! \n",
      "번역문장:   arrosez les plantes ! <sos> <sos> <sos> <sos> <sos>\n",
      "--------------------------------------------------\n",
      "입력문장:  go to bed . \n",
      "정답문장:  <sos> allez au lit ! \n",
      "번역문장:   va au lit ! <sos> <sos> <sos> <sos> <sos> <sos> <sos>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# seq_to_tar 함수 not working well!!! ===> 수정 필요\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(\"입력문장: \", seq_to_src(encoder_input_train[seq_index]))\n",
    "    print('정답문장: ', seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print('번역문장: ', decoded_sentence)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decpder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
