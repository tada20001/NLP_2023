{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY4TGidt9lb4uUmvFnK/lG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tada20001/NLP_2023/blob/main/CH09_08_%EC%82%AC%EC%A0%84%ED%9B%88%EB%A0%A8%EB%90%9C_%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9(Pre_trained_Word_Embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스 임베딩층과 사전훈련된 워드임베딩을 이용하여 비교\n",
        "\n",
        "### 1. 케라스 임베딩층(Keras Embedding layer)\n",
        "----------------\n",
        "케라스는 훈련데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공함\n",
        "\n",
        "Embedding()은 인공신경망 구조의 관점에서 임베딩층을 구현함\n",
        "\n",
        "#### **임베딩층은 룩업 테이블임**\n",
        "\n",
        "임베딩층의 입력으로 사용하기 위해 입력시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 함\n",
        "\n",
        "단어 --> 단어에 부여된 고유한 정수값 --> 임베딩층 통과 --> 밀집벡터\n",
        "\n",
        "임베딩층은 입력정수에 대해 밀집벡터로 맵핑하고 이 밀집벡터는 인공신경망의 학습과정에서 가중치가 학습되는 것과 같은 방식으로 훈련함\n",
        "\n",
        "훈련과정에서 단어는 모델이 풀고자 하는 작업에 맞는 값으로 업데이트됨. 그리고 이 밀집벡터를 임베딩 벡터라고 함\n",
        "\n",
        "정수를 밀집벡터 또는 임베딩 벡터로 맵핑한다는 것은, 특정단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터값을 가져오는 룩업 테이블이라고 할 수 있음\n",
        "\n",
        "그리고 이 테이블은 단어집합의 크기만큼 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가짐\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Q16IjguL5xpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 그림과 같이 단어 'great'을 정수 인코딩한 후 테이블로부터 해당 인덱스에 위치한 임베딩 벡터를 꺼내옴\n",
        "\n",
        "위의 그림에서는 임베딩 벡터 차원이 4로 설정되어 있고, 단어 great은 정수 인코딩 과정에서 1,918의 정수로 인코딩 되었고, 그에 따라 단어 집합의 크기만큼의 행을 가지는 테이블에서 인덱스 1,918번에 위치한 행을 단어 great의 임베딩 벡터로 사용함\n",
        "\n",
        "이 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습됨\n",
        "\n",
        "케라스는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 변환한 후 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인코딩까지만 진행한 후 임베딩 층의 입력으로 사용하여 룩업테이블 결과인 임베딩 벡터를 리턴함"
      ],
      "metadata": {
        "id": "QyNwaPy_871N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ez1yFfAI1nuN"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "output_dim = 128  # 임베딩 후 벡터 차원원\n",
        "input_length = 500  # 입력시퀀스 길이. 샘플길이가 500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding()은 (number of samples, input_length)인 2D 정수텐서를 입력으로 받음. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimentionality)인 3D 실수 텐서를 리턴함\n",
        "\n",
        "\n",
        "#### **임베딩층 사용하기**\n",
        "문장의 긍/부정을 판단하는 감성분류 모델 만들기"
      ],
      "metadata": {
        "id": "lNFkWN_UASck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "9DmmNFWa-23N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성분류 입력데이터터\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "-JnMdCLcBndV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스의 토크나이저를 사용하여 단어집합을 만들고 그 크기를 확인함"
      ],
      "metadata": {
        "id": "iuLHIx1-B7FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩 고려 +1\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLTgpcu4B52c",
        "outputId": "72d4c28e-49f9-455c-cd51-4aeb1e7ca376"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 문장에 대해 정수 인코딩 수행\n",
        "x_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(x_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egQfowt2CZjh",
        "outputId": "e5542e03-c13f-41a3-ef2f-5964d527c98a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 긴 문장 길이 구함\n",
        "max_len = max(len(sent) for sent in x_encoded)\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2K5z0KCsZu",
        "outputId": "e7c54413-c548-4350-d67c-1cae20244839"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이로 모든 샘플에 대한 패딩 실시\n",
        "X_train = pad_sequences(x_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNr1FvqUDFxb",
        "outputId": "8808dc93-f5eb-42db-d85d-0e44bee38ccb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgQe6k0eDcQa",
        "outputId": "aa3568db-e0fa-4201-b26f-6b053ff57b98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이진분류 모델 설계.출력층에 1개의 뉴런을 배치하고 활성화함수로 시그모이드 함수, 손실함수로 binary_crossentropy 사용"
      ],
      "metadata": {
        "id": "Q-bzoSY1Dehf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4  # 아웃풋 임베딩\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtm2fvKGEJOV",
        "outputId": "201f35ae-e461-42cd-ca21-8141cd51b743"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 2s - loss: 0.6919 - acc: 0.5714 - 2s/epoch - 2s/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6909 - acc: 0.5714 - 12ms/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6900 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6891 - acc: 0.5714 - 13ms/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6881 - acc: 0.5714 - 11ms/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6872 - acc: 0.5714 - 11ms/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6863 - acc: 0.5714 - 10ms/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6853 - acc: 0.5714 - 12ms/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6844 - acc: 0.5714 - 20ms/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6834 - acc: 0.5714 - 12ms/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6825 - acc: 0.7143 - 11ms/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6815 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6806 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6796 - acc: 0.7143 - 11ms/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6786 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6777 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6767 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6757 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6747 - acc: 0.7143 - 14ms/epoch - 14ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6737 - acc: 0.7143 - 12ms/epoch - 12ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6726 - acc: 0.7143 - 11ms/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6716 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6706 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6695 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6684 - acc: 0.8571 - 10ms/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6674 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6663 - acc: 0.8571 - 39ms/epoch - 39ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6652 - acc: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6641 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6630 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6618 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6607 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6595 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6583 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6572 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6560 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6548 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6535 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6523 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6511 - acc: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6498 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6485 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6472 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6459 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6446 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6433 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6419 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6406 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6392 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6378 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6365 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6350 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6336 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6322 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6307 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6293 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6278 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6263 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6248 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6233 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6218 - acc: 1.0000 - 21ms/epoch - 21ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6202 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6187 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6171 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6155 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.6140 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.6124 - acc: 1.0000 - 19ms/epoch - 19ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.6107 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.6091 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.6075 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.6058 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.6042 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.6025 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.6008 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5991 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5974 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5956 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5939 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5922 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5904 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5886 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5869 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5851 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5833 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5815 - acc: 1.0000 - 26ms/epoch - 26ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5796 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5778 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5760 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5741 - acc: 1.0000 - 21ms/epoch - 21ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5722 - acc: 1.0000 - 32ms/epoch - 32ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5704 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5685 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5666 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5647 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5628 - acc: 1.0000 - 22ms/epoch - 22ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5609 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5589 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5570 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5551 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5531 - acc: 1.0000 - 10ms/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f345526d8e0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기\n",
        "--------------------------\n",
        "훈련 데이터가 적은 경우에는 해당 문제에 특화된 것은 아니지만, 많은 훈련데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있음\n",
        "\n",
        "\n",
        "* GloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
        "* Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ],
      "metadata": {
        "id": "S-SiAhQDF7Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "CX7EJYPiF3qk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 벡터 불러오기. 딕셔너리 구조로 가져와 벡터 개수 확인인\n",
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding='utf8')\n",
        "i=0\n",
        "for line in f:\n",
        "  word_vector = line.split()  # 단어별 임베딩벡터 데이터\n",
        "  word = word_vector[0]\n",
        "\n",
        "  #  array로 변환\n",
        "  word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "  embedding_dict[word] = word_vector_arr\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "G18OH5bTJrYc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_dict)  # 40만개 임베딩벡터 존재"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4BbPLtNLeJO",
        "outputId": "d1d5ed68-b33a-475f-ea0f-37a957f663ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'repectable' 임베딩벡터 값 확인\n",
        "embedding_dict['respectable']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E8CUhf5MfoG",
        "outputId": "2bdf61f8-ed59-4dfb-a3f5-3a4149ba5948"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.049773 ,  0.19903  ,  0.10585  ,  0.1391   , -0.32395  ,\n",
              "        0.44053  ,  0.3947   , -0.22805  , -0.25793  ,  0.49768  ,\n",
              "        0.15384  , -0.08831  ,  0.0782   , -0.8299   , -0.037788 ,\n",
              "        0.16772  , -0.45197  , -0.17085  ,  0.74756  ,  0.98256  ,\n",
              "        0.81872  ,  0.28507  ,  0.16178  , -0.48626  , -0.006265 ,\n",
              "       -0.92469  , -0.30625  , -0.067318 , -0.046762 , -0.76291  ,\n",
              "       -0.0025264, -0.018795 ,  0.12882  , -0.52457  ,  0.3586   ,\n",
              "        0.43119  , -0.89477  , -0.057421 , -0.53724  ,  0.25587  ,\n",
              "        0.55195  ,  0.44698  , -0.24252  ,  0.29946  ,  0.25776  ,\n",
              "       -0.8717   ,  0.68426  , -0.05688  , -0.1848   , -0.59352  ,\n",
              "       -0.11227  , -0.57692  , -0.013593 ,  0.18488  , -0.32507  ,\n",
              "       -0.90171  ,  0.17672  ,  0.075601 ,  0.54896  , -0.21488  ,\n",
              "       -0.54018  , -0.45882  , -0.79536  ,  0.26331  ,  0.18879  ,\n",
              "       -0.16363  ,  0.3975   ,  0.1099   ,  0.1164   , -0.083499 ,\n",
              "        0.50159  ,  0.35802  ,  0.25677  ,  0.088546 ,  0.42108  ,\n",
              "        0.28674  , -0.71285  , -0.82915  ,  0.15297  , -0.82712  ,\n",
              "        0.022112 ,  1.067    , -0.31776  ,  0.1211   , -0.069755 ,\n",
              "       -0.61327  ,  0.27308  , -0.42638  , -0.085084 , -0.17694  ,\n",
              "       -0.0090944,  0.1109   ,  0.62543  , -0.23682  , -0.44928  ,\n",
              "       -0.3667   , -0.21616  , -0.19187  , -0.032502 ,  0.38025  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict['respectable'].shape  # embedding_dim = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMx01LLXMxw5",
        "outputId": "222ddb3e-b414-43ad-baf4-2d3b55ff4d9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :', np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anYOr3VvM13H",
        "outputId": "00dcd24b-64fb-425f-adb8-957f773573ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 데이터의 각 단어와 맵핑된 정수값 확인\n",
        "tokenizer.word_index.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxqB2fImOZBc",
        "outputId": "ef35ad05-5ae2-47cf-80c2-9c1b7981b69d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'great'의 경우 정수 2로 맵핑됨. 사전 훈련된 벡터값으로는 다음과 같음"
      ],
      "metadata": {
        "id": "nbhlZ4HPQoS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict['great']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuTJ9VNPQRpj",
        "outputId": "44999b28-e101-44ec-dfd6-a4388783e296"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786 ,  0.38216  ,  0.53236  ,  0.15261  , -0.29694  ,\n",
              "       -0.20558  , -0.41846  , -0.58437  , -0.77355  , -0.87866  ,\n",
              "       -0.37858  , -0.18516  , -0.128    , -0.20584  , -0.22925  ,\n",
              "       -0.42599  ,  0.3725   ,  0.26077  , -1.0702   ,  0.62916  ,\n",
              "       -0.091469 ,  0.70348  , -0.4973   , -0.77691  ,  0.66045  ,\n",
              "        0.09465  , -0.44893  ,  0.018917 ,  0.33146  , -0.35022  ,\n",
              "       -0.35789  ,  0.030313 ,  0.22253  , -0.23236  , -0.19719  ,\n",
              "       -0.0053125, -0.25848  ,  0.58081  , -0.10705  , -0.17845  ,\n",
              "       -0.16206  ,  0.087086 ,  0.63029  , -0.76649  ,  0.51619  ,\n",
              "        0.14073  ,  1.019    , -0.43136  ,  0.46138  , -0.43585  ,\n",
              "       -0.47568  ,  0.19226  ,  0.36065  ,  0.78987  ,  0.088945 ,\n",
              "       -2.7814   , -0.15366  ,  0.01015  ,  1.1798   ,  0.15168  ,\n",
              "       -0.050112 ,  1.2626   , -0.77527  ,  0.36031  ,  0.95761  ,\n",
              "       -0.11385  ,  0.28035  , -0.02591  ,  0.31246  , -0.15424  ,\n",
              "        0.3778   , -0.13599  ,  0.2946   , -0.31579  ,  0.42943  ,\n",
              "        0.086969 ,  0.019169 , -0.27242  , -0.31696  ,  0.37327  ,\n",
              "        0.61997  ,  0.13889  ,  0.17188  ,  0.30363  , -1.2776   ,\n",
              "        0.044423 , -0.52736  , -0.88536  , -0.19428  , -0.61947  ,\n",
              "       -0.10146  , -0.26301  , -0.061707 ,  0.36627  , -0.95223  ,\n",
              "       -0.39346  , -0.69183  , -1.0426   ,  0.28855  ,  0.63056  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인"
      ],
      "metadata": {
        "id": "X8qP1pVSQ4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "  vector_value = embedding_dict.get(word)\n",
        "  if vector_value is not None:\n",
        "    embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "euvj4TpOQ0qZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv-J0JWhRqdk",
        "outputId": "c6df0262-67ac-4ae1-a79f-8fb7d2c5291b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH3pPupDR-l9",
        "outputId": "15dc2e28-d4af-4eb2-b2b5-379886474b24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.8485 - acc: 0.4286 - 557ms/epoch - 557ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.8240 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.8002 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.7772 - acc: 0.4286 - 7ms/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.7550 - acc: 0.4286 - 7ms/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.7336 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.7130 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6932 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6742 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6560 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6384 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6216 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6054 - acc: 0.5714 - 10ms/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.5899 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.5749 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.5605 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.5467 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.5333 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.5204 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.5079 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4958 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4841 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4727 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.4618 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.4511 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.4407 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.4307 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.4209 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.4114 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.4022 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3932 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3845 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3761 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3678 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3598 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3521 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3445 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.3372 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.3300 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.3231 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.3164 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.3098 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.3034 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2973 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2912 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2854 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2797 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2742 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2688 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2636 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2585 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2535 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2487 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2440 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2395 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2351 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2307 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2265 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2225 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2185 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.2146 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.2108 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.2071 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.2036 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.2001 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1967 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1933 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1901 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1869 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1839 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1809 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1779 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1751 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1723 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1696 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1669 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1643 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1618 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1593 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1569 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1546 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1523 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1500 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1478 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1457 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1436 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1415 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1395 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1375 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1356 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1337 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1319 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1301 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1283 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1266 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1249 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1233 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1217 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1201 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1185 - acc: 1.0000 - 6ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f34547d8940>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **사전 훈련된 Word2Vec 사용하기**"
      ],
      "metadata": {
        "id": "kbwc0jeaUwn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "AynLca4SUZMv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVMOSWgjcCS_",
        "outputId": "e938980c-8257-4bfd-b684-c6882d432c96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\", filename=\"GoogleNews-vectors-negative300.bin.gz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIaosHNAU3Tf",
        "outputId": "11a653a6-01b8-4845-d7b9-88ef0f362cec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('GoogleNews-vectors-negative300.bin.gz',\n",
              " <http.client.HTTPMessage at 0x7f3446fbec70>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "pbwoOTolU5mv",
        "outputId": "8890b42e-7337-497d-8474-06b07797af5e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadGzipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-803c04fd48b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             limit=limit, datatype=datatype)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\037\\213'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not a gzipped file (%r)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         (method, flag,\n",
            "\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'<!')"
          ]
        }
      ]
    }
  ]
}