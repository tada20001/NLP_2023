{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb6cd3d",
   "metadata": {},
   "source": [
    "* BPE와 유사한 알고리즘인 Wordpiece Model 채택\n",
    "* 패키지를 통해 간단하게 단어들을 서브워드로 분리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed55fcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabac32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "     ---------------------------------------- 5.3/5.3 MB 19.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: toml in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.20.1)\n",
      "Requirement already satisfied: termcolor in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-1.0.0-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 146.5/146.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 101.4/101.4 kB ? eta 0:00:00\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.3/52.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.21.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.3.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.2.0)\n",
      "Requirement already satisfied: zipp in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.8.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->tensorflow_datasets) (0.4.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ------------------------------------- 223.0/223.0 kB 13.3 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.12.2\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl (904 kB)\n",
      "     ------------------------------------- 904.4/904.4 kB 19.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=e90f0ba07bbcbb6fb8d5e265b8d60c6e968f2fcb08f8ff97cb781ab2d4707a53\n",
      "  Stored in directory: c:\\users\\tada2\\appdata\\local\\pip\\cache\\wheels\\54\\aa\\01\\724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, protobuf, promise, etils, googleapis-common-protos, tensorflow-metadata, tensorflow_datasets\n",
      "Successfully installed dm-tree-0.1.8 etils-1.0.0 googleapis-common-protos-1.58.0 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.12.0 tensorflow_datasets-4.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tfds.exe is installed in 'C:\\Users\\tada2\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.3.0 requires gast==0.3.3, but you have gast 0.4.0 which is incompatible.\n",
      "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
      "tensorflow 2.3.0 requires scipy==1.4.1, but you have scipy 1.9.3 which is incompatible.\n",
      "tensorflow 2.3.0 requires tensorflow-estimator<2.4.0,>=2.3.0, but you have tensorflow-estimator 2.6.0 which is incompatible.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca69e7c",
   "metadata": {},
   "source": [
    "### 1. IMDB 리뷰 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d34b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41603f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드 및 저장\n",
    "train_df = pd.read_csv('IMDB_Reviews.csv')\n",
    "train_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630114a1",
   "metadata": {},
   "source": [
    "* tfds.deprecated.text.SubwordTextEncoder.build_from_corpus의 인자로 토큰화할 데이터를 넣어 줌\n",
    "* 이 작업을 통해 서브워드로 이루어진 단어집합을 생성하고, 각 서브워드에 고유한 정수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a279101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3637da",
   "metadata": {},
   "source": [
    "* .subwords를 통해 토큰화된 서브워드 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bd4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7b05a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 20번 인텍스의 샘플 출력, 정수인코딩한 결과 비교\n",
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b270295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b796c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장: [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n"
     ]
    }
   ],
   "source": [
    "# train_df에 존재하는 문장 중 일부 발췌\n",
    "sample_string =  \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "# 인코딩 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후의 문장: {}'.format(tokenized_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4604c36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 문장 : It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "# 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff096c05",
   "metadata": {},
   "source": [
    "* .vocab_size를 통해 단어집합의 크기 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58aa797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8268\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d8ecb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ---> It\n",
      "8051 ---> '\n",
      "8 ---> s \n",
      "910 ---> mind\n",
      "8057 ---> -\n",
      "2169 ---> blow\n",
      "36 ---> ing \n",
      "7 ---> to \n",
      "103 ---> me \n",
      "13 ---> that \n",
      "14 ---> this \n",
      "32 ---> film \n",
      "18 ---> was \n",
      "79 ---> even \n",
      "681 ---> made\n",
      "8058 ---> .\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수 확인\n",
    "for ts in tokenized_string:\n",
    "    print('{} ---> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9618ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n"
     ]
    }
   ],
   "source": [
    "# 기존 예제 문장 중 even 이라는 단어에 임의로 xyz 글자를 추가\n",
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후의 문장 : {}'.format(tokenized_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a38a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 --> It\n",
      "8051 --> '\n",
      "8 --> s \n",
      "910 --> mind\n",
      "8057 --> -\n",
      "2169 --> blow\n",
      "36 --> ing \n",
      "7 --> to \n",
      "103 --> me \n",
      "13 --> that \n",
      "14 --> this \n",
      "32 --> film \n",
      "18 --> was \n",
      "7974 --> even\n",
      "8132 --> x\n",
      "8133 --> y\n",
      "997 --> z \n",
      "681 --> made\n",
      "8058 --> .\n"
     ]
    }
   ],
   "source": [
    "for token_num in tokenized_string:\n",
    "    print(\"{} --> {}\".format(token_num, tokenizer.decode([token_num])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29699757",
   "metadata": {},
   "source": [
    "* x, y, z에 대한 서브워드가 각각 분리되어 추가됨\n",
    "\n",
    "### 2. 네이버 영화 리뷰 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c21346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f95c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f519bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null 값 확인 후 제거\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef8ea2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.dropna(how='any')\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18414d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data['document'], target_vocab_size=2**13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab8f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 100개의 서브워드 출력\n",
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10e6783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\n"
     ]
    }
   ],
   "source": [
    "# encode를 통해 사례 테스트\n",
    "sample = train_data['document'][20]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9410b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de7520d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보면서 웃지 않는 건 불가능하다\n"
     ]
    }
   ],
   "source": [
    "sample = train_data['document'][21]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3956cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [570, 892, 36, 584, 159, 7091, 201]\n",
      "기존 문장 : 보면서 웃지 않는 건 불가능하다\n"
     ]
    }
   ],
   "source": [
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample)\n",
    "print(\"정수 인코딩 후의 문장 : {}\".format(tokenized_string))\n",
    "\n",
    "# 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96fd1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 --> 보면서 \n",
      "892 --> 웃\n",
      "36 --> 지 \n",
      "584 --> 않는 \n",
      "159 --> 건 \n",
      "7091 --> 불가능\n",
      "201 --> 하다\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print(\"{} --> {}\".format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be2079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
