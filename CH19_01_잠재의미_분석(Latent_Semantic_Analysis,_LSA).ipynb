{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYFE6K0SqDPHqD4pSvbYra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tada20001/NLP_2023/blob/main/CH19_01_%EC%9E%A0%EC%9E%AC%EC%9D%98%EB%AF%B8_%EB%B6%84%EC%84%9D(Latent_Semantic_Analysis%2C_LSA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토픽모델링(LDA)은 토픽이라는 문서집합에서 주제를 도출하기 위한 통계모델 중 하나, 텍스트 본문에서 숨겨진 의미구조를 발견하기 위해 사용되는 텍스트 마이닝 기법\n",
        "\n",
        "\n",
        "LSA는 토픽모델링을 위해 최적화된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘임. \n",
        "\n",
        "BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있음. 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재의미분석(LSA, Latent Semantic Analysis) 방법이 있음. 잠재의미분석(Latent Semantic Indexing, LSI)이라고 부르기도 함\n",
        "\n",
        "이 방법을 이해하기 위해서는 선형대수학의 특이값 분해(Singulary Value Decomposition, SVD)를 이해해야 함. \n",
        "\n",
        "\n",
        "### 1. 특이값 분해(Singular Value Decomposition, SVD)\n",
        "--------------\n",
        "SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말함.\n",
        "\n",
        "$$A=UΣV^\\text{T}$$\n",
        "\n",
        "여기서 각 3개의 행렬은 다음의 조건을 만족해야 함\n",
        "* $U: m × m\\ \\text{직교행렬}\\ (AA^\\text{T}=U(ΣΣ^\\text{T})U^\\text{T})$\n",
        "* $V: n × n\\ \\text{직교행렬}\\ (A^\\text{T}A=V(Σ^\\text{T}Σ)V^\\text{T})$\n",
        "* $Σ: m × n\\ \\text{직사각 대각행렬}$"
      ],
      "metadata": {
        "id": "vK7IG4ZmI92f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 직교행렬(orthogonal matrix)이란, 자신과 자신의 전치행렬(transposed matrix)의 곱 또는 이를 반대로 곱한 결과가 단위행렬(identity matrix)이 되는 행렬을 의미함. \n",
        "\n",
        "이때 SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값(singular value)라고 함\n",
        "\n",
        "#### 1) 전치행렬(Transposed Matrix)\n",
        "전치행렬(transposed matrix)은 원래의 행렬에서 행과 열을 바꾼 행렬임. 즉, 주 대각선을 축으로 반사대칭을 하는 행렬임.\n",
        "\n",
        "\n",
        "#### 2) 단위 행렬(Identity Matrix)\n",
        "단위행렬은 주대각선의 원소가 모두 1이며 나머지 원소는 0인 정사각 행렬을 말함\n",
        "\n",
        "#### 3) 역행렬(Inverse Matrix)\n",
        "행렬 $A$와 어떤 행렬을 곱했을 때, 결과로서 단위행렬이 나온다면 이때의 어떤 행렬을 A의 역행렬이라고 하며 $A^{-1}$라고 함\n",
        "\n",
        "$$A\\ ×\\ A^{-1} = I$$\n",
        "\n",
        "\n",
        "#### 4) 직교행렬(Orthogonal matrix)\n",
        "실수 $n × n$ 행렬 $A$에 대해 $A\\ ×\\ A^{T} = I$를 만족하면서 $A^{T}\\ ×\\ A = I$를 만족하는 행렬 $A$를 직교행렬이라고 함. 그런데 역행렬의 정의를 다시 생각해보면, 직교행렬은 $A^{-1}=A^{T}$를 만족함\n",
        "\n",
        "\n",
        "#### 5) 대각 행렬(Diagonal matrix)\n",
        "대각행렬은 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 말함. 아래의 그림에서는 주대각선의 원소를 $a$라고 표현하고 있음\n",
        "\n",
        "\n",
        "---------\n",
        "여기에서.. SVD를 통해 나온 대각행렬 Σ은 추가적인 성질을 가지는데, 대각행렬 Σ의 주대각 원소를 행렬 A의 특이값(singular value)라고 하며, 이를 ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFEAAAATCAIAAABnfXYGAAACXklEQVRYhe2XvetxYRjHz3mwMPkLlAwGxyQGRZLsBh05ZeEvMElsRmWSMshLBqtMBuUtFB0DYpBEDqHknXI/gzrOw+M4/NRv4DPdL9f9/V7Xue57ODAAAPow/vx2Ar/At+bP4FvzZ8C+mq/Xa7/fXywWR6PR8XgEAEgkkkgk8po6qUYQxOFwAAAgCBIOh3+c9rVFoVAgCIJpwoBCp9PRarVer3c8HqfTaZfLBX7Ae9XoLSaTCXOLf2q2WCxut5ucIghSq9VeTui9am+0uLzn8Xicz+dNJhO5stlslsvleXw8Hh0OR7FYZHjlaNQajQaGYbFYTKPRdLtdxrf4CQt6Lu+51+tBECQWi8/TXC7HYrEQBIEgaDabJRIJHo/HZl+//3tcqWWzWVLt3B+dTrff7zOZjFAoZKhJb0FN+AFkx0+nk0gk6vf7AIDtdothWDgcpl4Jj8dTLpfPY61WG4vFaO7PQzUAgNPpzOfzt2ooivr9fgAAQRAoiqZSKQBAqVRCUbTZbJLxVIvdbke16PV6er0+Go0ajcbpdHrle+kbDMOhUMjlcolEosFgYLPZVCrVvS8FwzCHw6H5lA/VWq3W6XRSKpW3au12mwxut9vUdao7jYVAIFitVnK53GAwcLncu31+CNnn1Wolk8nm8znzs7dgGHYePKvGJH6325nN5nu7TN8nFafTGQwG+Xz+C2fP2O32QCAAQVA8Hq9UKk+pMXHHcVyhUNzbZVpzMpnEcXy/3y8WC5/Px/DUf6lWq8Ph0Gq1brdbtVr9rBqT+Hq9LpVK7+3C4Pv//Al8Ys1/ASreUMmklKO7AAAAAElFTkSuQmCC)라고 할때, 특이값 ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFEAAAATCAIAAABnfXYGAAACXklEQVRYhe2XvetxYRjHz3mwMPkLlAwGxyQGRZLsBh05ZeEvMElsRmWSMshLBqtMBuUtFB0DYpBEDqHknXI/gzrOw+M4/NRv4DPdL9f9/V7Xue57ODAAAPow/vx2Ar/At+bP4FvzZ8C+mq/Xa7/fXywWR6PR8XgEAEgkkkgk8po6qUYQxOFwAAAgCBIOh3+c9rVFoVAgCIJpwoBCp9PRarVer3c8HqfTaZfLBX7Ae9XoLSaTCXOLf2q2WCxut5ucIghSq9VeTui9am+0uLzn8Xicz+dNJhO5stlslsvleXw8Hh0OR7FYZHjlaNQajQaGYbFYTKPRdLtdxrf4CQt6Lu+51+tBECQWi8/TXC7HYrEQBIEgaDabJRIJHo/HZl+//3tcqWWzWVLt3B+dTrff7zOZjFAoZKhJb0FN+AFkx0+nk0gk6vf7AIDtdothWDgcpl4Jj8dTLpfPY61WG4vFaO7PQzUAgNPpzOfzt2ooivr9fgAAQRAoiqZSKQBAqVRCUbTZbJLxVIvdbke16PV6er0+Go0ajcbpdHrle+kbDMOhUMjlcolEosFgYLPZVCrVvS8FwzCHw6H5lA/VWq3W6XRSKpW3au12mwxut9vUdao7jYVAIFitVnK53GAwcLncu31+CNnn1Wolk8nm8znzs7dgGHYePKvGJH6325nN5nu7TN8nFafTGQwG+Xz+C2fP2O32QCAAQVA8Hq9UKk+pMXHHcVyhUNzbZVpzMpnEcXy/3y8WC5/Px/DUf6lWq8Ph0Gq1brdbtVr9rBqT+Hq9LpVK7+3C4Pv//Al8Ys1/ASreUMmklKO7AAAAAElFTkSuQmCC)은 내림차순으로 정렬되는 특징을 가짐\n",
        "\n"
      ],
      "metadata": {
        "id": "ED-VBr5oNNyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 절단된 SVD(Truncated SVD)\n",
        "-------------\n",
        "위에서 설명한 SVD를 full SVD라고 하지만, LSA의 경우 full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제한 절단된 SVD(truncated SVD)를 사용하게 됨\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "1vZZvegedFVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "절단된 SVD는 대각 행렬 Σ의 대각 원소 값 중에서 상위 값 t개만 남게 됨. 절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A로 복구할 수 없음. \n",
        "\n",
        "또한, U행렬과 V행렬의 t열까지만 남기게 됨. 여기에서 t는 우리가 찾고자 하는 토픽의수를 반영한 하이퍼파라미터 값이 됨. \n",
        "\n",
        "그러나 t를 선택하는 것은 쉽지 않은 일인데, t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 얻을 수 있지만, t를 갖게 잡아야만 노이즈를 제거할 수 있음.\n",
        "\n",
        "이렇게 일부 벡터들을 삭제하는 것을 데이터의 차원을 줄인다고도 하는데, 데이터의 차원을 줄이게 되면 당연히 full SVD보다 직관적으로 계산비용이 낮아지는 효과도 얻을 수 있음.\n",
        "\n",
        "계산비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과도 갖고 있음. 이는 영상처리 분야에서는 노이즈를 제거한다는 의미를 갖고 있고, 자연어처리 분야에서는 설명력이 낮은 정보를 삭제한다는 의미를 가짐. 이 정보를 잠재의미로 부름\n",
        "\n",
        "### 3. 잠재의미분석(Latent Semantic Analysis, LSA)\n",
        "----------------\n",
        "기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있음. LSA는 기본적으로 DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있음.\n",
        "\n",
        "#### 1) Full SVD\n",
        "\n"
      ],
      "metadata": {
        "id": "f6k-au7MdcUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jYAcB1JWB9lw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예를 들어..."
      ],
      "metadata": {
        "id": "c69SzCwvm8fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [0, 0, 0, 1, 0, 1, 1, 0, 0], \n",
        "    [0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
        "    [0, 1, 1, 0, 2, 0, 0, 0, 0],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
        "])\n",
        "\n",
        "print('DTM 크기(shape):', A.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAYB9mbZm7tY",
        "outputId": "b4030f4f-2799-4659-9081-08453e5e3a9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTM 크기(shape): (4, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full SVD 수행\n",
        "## 대각행렬 변수명을 S로 사용, V의 전치행렬은 VT로 함"
      ],
      "metadata": {
        "id": "_nToPMvLnfXq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, s, VT = np.linalg.svd(A, full_matrices=True)\n",
        "print('행렬 U :')\n",
        "print(U.round(2))\n",
        "print('행렬 U의 크기(shape):', np.shape(U))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sajkR0ipUlc",
        "outputId": "6c8ba700-44eb-4268-d6d2-b433700a41ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "행렬 U :\n",
            "[[-0.24  0.75  0.   -0.62]\n",
            " [-0.51  0.44 -0.    0.74]\n",
            " [-0.83 -0.49 -0.   -0.27]\n",
            " [-0.   -0.    1.    0.  ]]\n",
            "행렬 U의 크기(shape): (4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('특이값 벡터 :')\n",
        "print(s.round(2))\n",
        "print('특이값 벡터의 크기(shape):', np.shape(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsEQcJqlqlPK",
        "outputId": "fd76d74c-8564-49c5-d9ea-665eee512ceb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특이값 벡터 :\n",
            "[2.69 2.05 1.73 0.77]\n",
            "특이값 벡터의 크기(shape): (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy의 linalg.svd()는 특이값 분해 결과로 대각행렬이 아니라, 특이값 리스트를 반환함. 따라서 이를 다시 대각 행렬로 바꿔줘야 함."
      ],
      "metadata": {
        "id": "xTF0plBKupVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 대각 행렬의 크기인 4 x 9의 임의행렬을 생성\n",
        "S = np.zeros((4, 9))\n",
        "\n",
        "# 특이값을 대각행렬에 삽입\n",
        "S[:4, :4] = np.diag(s)\n",
        "\n",
        "print('대각 행렬 S :')\n",
        "print(S.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kILLnD70riT4",
        "outputId": "4175a89b-ea79-4609-c203-aeef9392603b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대각 행렬 S :\n",
            "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 x 9의 크기를 가지는 대각 행렬 S가 생성되었으며, 대각 행렬이 내림차순으로 나열되어 있음"
      ],
      "metadata": {
        "id": "3WXOrUlevegP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('직교행렬 VT:')\n",
        "print(VT.round(2))\n",
        "\n",
        "print('직교행렬 VT의 크기(shape) :')\n",
        "print(VT.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tltIn4nvaTf",
        "outputId": "0a86eceb-b488-4f1f-e764-2d1c82863ad5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "직교행렬 VT:\n",
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
            " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
            " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
            " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
            " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
            " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
            "직교행렬 VT의 크기(shape) :\n",
            "(9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9 x 9 크기를 가지는 직교행렬 VT(V의 전치행렬)이 생성되었음. 즉 U x S x VT를 하면 기존 행렬 A가 나와야 함. Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴함. 이를 사용하여 정말로 기존의 행렬 A와 동일한지 확인해 봄."
      ],
      "metadata": {
        "id": "ji9JS7J2v5uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.allclose(A, np.dot(np.dot(U, S), VT).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZlJb9yJvy0C",
        "outputId": "61df2d84-f9f9-49d5-c892-951b509a452d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 절단된 SVD(Truncated SVD)\n",
        "\n",
        "이제 t를 정하고 절단된 SVD(Truncated SVD)를 수행함. 여기서는 t=2로 대각 행렬 S내의 특이값 중에서 상위 2개만 남기고 제거하도록 함"
      ],
      "metadata": {
        "id": "gtlvOS9nwgT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특이값 상위 2개만 보존\n",
        "S = S[:2, :2]\n",
        "S.round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew0xTqC2wYUS",
        "outputId": "ecdc9d13-ebd6-4364-9e94-621c514fbc7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.69, 0.  ],\n",
              "       [0.  , 2.05]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 2개의 값만 남기고 나머지는 모두 제거\n",
        "U = U[:, :2]\n",
        "print('행렬 U:')\n",
        "print(U.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKOMubc_xKnq",
        "outputId": "738f3ac6-3146-4459-ee38-3be49c268916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "행렬 U:\n",
            "[[-0.24  0.75]\n",
            " [-0.51  0.44]\n",
            " [-0.83 -0.49]\n",
            " [-0.   -0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2개의 열만 남기고 모두 제거됨. 이제 행렬 V의 전치 행렬인 VT에 대해 2개의 행만 남기고 제거함."
      ],
      "metadata": {
        "id": "Ylp9oi6O96OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VT = VT[:2, :]\n",
        "print('직교행렬 VT :')\n",
        "print(VT.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHR8DFD24NaQ",
        "outputId": "af66300a-806e-4b3e-b90a-d54f7bd606ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "직교행렬 VT :\n",
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_prime = np.dot(np.dot(U, S), VT)\n",
        "print(A)\n",
        "print(A_prime.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUMLudcnDvrk",
        "outputId": "cbef06f5-4445-465d-ace7-eff33a330a5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 1 1 0 0]\n",
            " [0 0 0 1 1 0 1 0 0]\n",
            " [0 1 1 0 2 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 1]]\n",
            "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
            " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
            " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
            " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대체로 기존에 0인 값들은 0에 가까운 값으로 대체된 것으로 볼 수 있음. 또한, 값이 제대로 복구되지 않은 구간도 존재함. \n",
        "\n",
        "이제 이렇게 차원이 축소된 U, S, VT의 크기가 어떤 의미를 갖고 있는지 알아보자.\n",
        "\n",
        "축소된 U는 4 x 2의 크기를 가지는데, 이는 문서의 갯수 x 토픽의 수 t의 크기임. 단어의 개수인 9는 유지되지 않는데, 문서의 개수인 4의 크기는 유지됨. \n",
        "\n",
        "이 문서 벡터들과 단어벡터들을 통해 다른 문서와의 유사도, 다른 단어와의 유사도, 단어(쿼리)로부터 문서의 유사도를 구할 수 있음\n",
        "\n",
        "\n",
        "### 4. 실습을 통한 이해\n",
        "-----------\n",
        "scikitleran에서 Twenty Newsgroup의 20개 주제의 뉴스그룹 데이터셋을 제공함. LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력하는 실습으로 토픽 모델링을 수행함\n",
        "\n",
        "#### 1) 뉴스그룹 데이터에 대한 이해"
      ],
      "metadata": {
        "id": "QnHiXjdbFxvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "h4ooHi1FFsry"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7bsZSuXHPHw",
        "outputId": "09e4113e-9f69-4aba-b7ed-abf33272acb3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "tyn7H0VsHiHH",
        "outputId": "adb42b9b-d100-4a91-8fd4-4c17f0435d54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "뉴스그룹 데이터에는 특수문자 포함된 다수의 영어문장으로 구성되어 있음. 이런 형식의 샘플이 총 11,314개 존재함. 그리고 target_name에는 뉴스그룹의 20개 카테고리가 저장되어 있음"
      ],
      "metadata": {
        "id": "xNpdfbfVHx5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhUpKL9JHns5",
        "outputId": "fdac0a63-090f-4450-b762-c3dfb6e0ab51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 텍스트 전처리\n",
        "\n",
        "시작하기 앞서, 텍스트 데이터에 대해 가능한 정제과정을 거쳐야 함. 기본적인 아이디어는 알파벳을 제외한 나머지를 제거하는 것임."
      ],
      "metadata": {
        "id": "oJo09hazIKpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document': documents})\n",
        "\n",
        "# 특수문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace('[^a-zA-Z]', ' ')\n",
        "\n",
        "# 길이가 3 이하인 단어 제거\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "kpxeSdPyIHII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba39f47-63c8-4f69-d909-353a5d854a72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-c7b7e365bfe1>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  news_df['clean_doc'] = news_df['document'].str.replace('[^a-zA-Z]', ' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제전, 후에 대한 차이 확인!!"
      ],
      "metadata": {
        "id": "4C6twBFG9aQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['clean_doc'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "alTZBykT9W2r",
        "outputId": "7a895f96-a9c6-411e-f4ec-476e2b173e07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 데이터에서 불용어를 제거함. 제거하기 위해 우선 토큰화를 우선 수행하였고, 토큰화와 불용어 제거를 순차적으로 진행함"
      ],
      "metadata": {
        "id": "H8eZTsoA9oz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E6WoUIu-hDJ",
        "outputId": "62bd6dab-0bb5-4bbf-f1c8-1ee18cc4d8bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NTTK로부터 불용어를 받아옴\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())  # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "metadata": {
        "id": "l7D_rj0c9ik-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인!!\n",
        "print(tokenized_doc[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXpw0_B2-bpU",
        "outputId": "618f5e3d-b29f-48f6-f18d-7088f5e6be4a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) TF-IDF 행렬 만들기\n",
        "\n",
        "불용어 제거를 위해 토큰화 작업을 수행했으나, TfidfVectorize는 기본적으로 토큰화가 되어 있지 않은 텍스트 데이터를 입력으로 사용함. \n",
        "\n",
        "따라서 TfidfVectorizer를 사용해서 TF-IDF 행렬을 만들기 위해 다시 토큰화 작업을 역으로 취소하는 작업을 수행함. 이를 역토큰화(Detokenization)라고 함."
      ],
      "metadata": {
        "id": "3xQwbQbw-08J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 역토큰화\n",
        "detokenized_doc = []\n",
        "\n",
        "for i in range(len(news_df)):\n",
        "  t = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(t)\n",
        "news_df['clean_doc'] = detokenized_doc"
      ],
      "metadata": {
        "id": "qCEbVgAI-xnL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인\n",
        "news_df['clean_doc'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Z_AgTeS6DNq4",
        "outputId": "fd8e8276-6438-4679-fc20-9343755a303e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어가 제거되었지만 토큰화는 수행되지 않은 형태가 됨.\n",
        "\n",
        "이제 TfidfVecotrizer를 통해 단어 1,000개에 대해 TF-IDF 행렬을 만들것임. 모든 단어가 아닌 1,000개만.."
      ],
      "metadata": {
        "id": "8-RDJnz3DZEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, max_df=0.5, smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "\n",
        "# TF-IDF 행렬 크기 확인\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZA_hMeqDSEB",
        "outputId": "1a5717fe-33b8-4420-9441-739d014ed052"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(news_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWBZaeVUEGrm",
        "outputId": "858477c6-2c52-465a-d832-7e32a778a152"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) 토픽 모델링(Topic Modeling)\n",
        "이제 TF-IDF 행렬을 다수의 행렬로 분해함. 여기에서는 사이킷런의 절단 SVD(Truncated SVD)를 사용함. 절단된 SVD를 사용하면 차원을 축소할 수 있음. 원래 기존 뉴스그룹 데이터가 20개의 카테고리를 갖고 있었기 때문에, 20개의 토픽을 가졌다고 가정하고 토픽 모델링을 시도함. 토픽의 숫자가 n_components의 파라미터로 지정이 가능함"
      ],
      "metadata": {
        "id": "ShFYG-gtENCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "len(svd_model.components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At41QrriELkE",
        "outputId": "16003cd8-06d3-44a1-8052-b385e770428d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 svd_model.components_는 앞서 배운 LSA에서 VT에 해당함"
      ],
      "metadata": {
        "id": "2EfAUXLDFPE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(svd_model.components_) # 정확하게 토픽의 수 t x 단어의 수의 크기를 갖고 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYbgOtGyFIaJ",
        "outputId": "6fce181d-380a-4034-f0c6-62c2d356b3fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out()  # 단어집합. 1,000개의 단어가 저장됨\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "  for idx, topic in enumerate(components):\n",
        "    print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]])"
      ],
      "metadata": {
        "id": "Sn237efDFYJR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 20개의 행 1,000개 열 중 가장 값이 큰 5개의 값을 찾아 단어로 출력함"
      ],
      "metadata": {
        "id": "2V1TuSOEvSRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_topics(svd_model.components_, terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBll6okrKpe7",
        "outputId": "95471ca8-6c74-48ef-938e-b2a1c5b26eb9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
            "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
            "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
            "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
            "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
            "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
            "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
            "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
            "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
            "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
            "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
            "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
            "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
            "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
            "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
            "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
            "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
            "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
            "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
            "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model.components_[0]\n",
        "len(svd_model.components_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7AMcX9jviaQ",
        "outputId": "750a553c-d818-48cd-f1a2-650fe91a56fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. LSA의 장단점\n",
        "----------------\n",
        "LSA는 쉽고 빠르게 구현이 가능할 뿐만 아니라 단어의 잠재적인 의미를 끌어낼 수 있음. 그러나 SVD 특성상 이미 계산된 LSA에 새로운 데이터를 추가하여 계산하면 처음부터 다시 계산해야 하는 단점이 있음. 즉, 새로운 정보에 대한 업데이트가 어려움. 최근에는 LSA 대산 Word2Vec 등 단어의 의미를 벡터화할 수 있는 방법, 인공신경망이 각광받고 있음"
      ],
      "metadata": {
        "id": "68CiV2-FwjmF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlNVA66XvsX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}